{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f840f",
   "metadata": {},
   "source": [
    "## Using FinBert to Analyze NKE, LULU, ATZ Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29d4a3",
   "metadata": {},
   "source": [
    "**Load Data (Yahoo Finance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  articles downloaded from yfinance for ticker NKE\n",
      "                                     id  \\\n",
      "0  353f79bb-28d4-3576-9604-ea027dbc1863   \n",
      "1  099cdb70-42ad-33dd-8710-67e58926300c   \n",
      "2  3521219f-b614-39c3-bf7c-d576239c10c7   \n",
      "3  fd56221d-e8af-3194-8936-31f4dfc34eed   \n",
      "4  c3f8eea9-6fdb-31ae-b81a-ab73c4adeaa0   \n",
      "\n",
      "                                             content  \n",
      "0  {'id': '353f79bb-28d4-3576-9604-ea027dbc1863',...  \n",
      "1  {'id': '099cdb70-42ad-33dd-8710-67e58926300c',...  \n",
      "2  {'id': '3521219f-b614-39c3-bf7c-d576239c10c7',...  \n",
      "3  {'id': 'fd56221d-e8af-3194-8936-31f4dfc34eed',...  \n",
      "4  {'id': 'c3f8eea9-6fdb-31ae-b81a-ab73c4adeaa0',...  \n",
      "NKE new successfuly converted to dataframe\n",
      "Successfully added NKE news to all_news_dataframe\n",
      "10  articles downloaded from yfinance for ticker LULU\n",
      "                                     id  \\\n",
      "0  2dbad12b-5395-3ee7-b817-315712ad780f   \n",
      "1  c8aa2195-917d-3b04-9c7d-329ca64c9588   \n",
      "2  fa4c4729-e5c3-34d3-9a32-8bb08ecbfe34   \n",
      "3  561a0c2e-1eb8-3441-887a-e67df5f3a8d9   \n",
      "4  ed8e4562-c627-3c33-91bb-beffdc7cbcb2   \n",
      "\n",
      "                                             content  \n",
      "0  {'id': '2dbad12b-5395-3ee7-b817-315712ad780f',...  \n",
      "1  {'id': 'c8aa2195-917d-3b04-9c7d-329ca64c9588',...  \n",
      "2  {'id': 'fa4c4729-e5c3-34d3-9a32-8bb08ecbfe34',...  \n",
      "3  {'id': '561a0c2e-1eb8-3441-887a-e67df5f3a8d9',...  \n",
      "4  {'id': 'ed8e4562-c627-3c33-91bb-beffdc7cbcb2',...  \n",
      "LULU new successfuly converted to dataframe\n",
      "Successfully added LULU news to all_news_dataframe\n",
      "10  articles downloaded from yfinance for ticker ATZ.TO\n",
      "                                     id  \\\n",
      "0  4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60   \n",
      "1  44127e76-4d51-3341-91f7-75a151c327de   \n",
      "2  d27b1692-49b5-37f9-8f7f-170607a64bc6   \n",
      "3  adf56cf4-fc14-38e2-8b57-4bdc4485cefa   \n",
      "4  45fdbd44-5798-37fc-9b78-dbf1bb62b6f0   \n",
      "\n",
      "                                             content  \n",
      "0  {'id': '4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60',...  \n",
      "1  {'id': '44127e76-4d51-3341-91f7-75a151c327de',...  \n",
      "2  {'id': 'd27b1692-49b5-37f9-8f7f-170607a64bc6',...  \n",
      "3  {'id': 'adf56cf4-fc14-38e2-8b57-4bdc4485cefa',...  \n",
      "4  {'id': '45fdbd44-5798-37fc-9b78-dbf1bb62b6f0',...  \n",
      "ATZ.TO new successfuly converted to dataframe\n",
      "Successfully added ATZ.TO news to all_news_dataframe\n",
      "         0\n",
      "0       id\n",
      "1  content\n",
      "2       id\n",
      "3  content\n",
      "4       id\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "TICKERS = [\"NKE\", \"LULU\", \"ATZ.TO\"]\n",
    "OUTPUT_FILE = \"financial_news_dataset.csv\"\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for tic in TICKERS:\n",
    "    # 1. Fetch\n",
    "    raw_news = yf.Ticker(tic).news\n",
    "    \n",
    "    # 2. Tag with Ticker\n",
    "    # We loop through the list to add the ticker symbol to each dict\n",
    "    for item in raw_news:\n",
    "        item['ticker'] = tic\n",
    "    \n",
    "    all_articles.extend(raw_news)\n",
    "    print(f\"Fetched {len(raw_news)} articles for {tic}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# 3. Create DataFrame\n",
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "if not df.empty:\n",
    "    # 4. Clean & Format\n",
    "    # Convert Unix timestamp to readable UTC date\n",
    "    df['date_utc'] = pd.to_datetime(df['providerPublishTime'], unit='s')\n",
    "    \n",
    "    # Rename 'title' -> 'text' for FinBERT compatibility\n",
    "    df = df.rename(columns={'title': 'text'})\n",
    "    \n",
    "    # Keep only the columns you actually need\n",
    "    keep_cols = ['ticker', 'date_utc', 'text', 'publisher', 'link']\n",
    "    final_df = df[keep_cols].sort_values(['ticker', 'date_utc'], ascending=[True, False])\n",
    "    \n",
    "    # 5. Save\n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nSUCCESS: Saved {len(final_df)} rows to '{OUTPUT_FILE}'\")\n",
    "    display(final_df.head())\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1005cf",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b39a6",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProsusAI/finbert loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_NAME = \"ProsusAI/finbert\" #tabularisai/ModernFinBERT\" or \"ProsusAI/finbert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "id2label = model.config.id2label  # e.g. {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "print(f\"{MODEL_NAME} loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be66d6",
   "metadata": {},
   "source": [
    "**Scoring Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5914dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_score(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = torch.softmax(logits, dim=-1).numpy()\n",
    "    labels = [id2label[int(i)] for i in probs.argmax(axis=1)]\n",
    "    return labels, probs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
