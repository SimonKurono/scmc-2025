{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f840f",
   "metadata": {},
   "source": [
    "## Load Data from Yfinance, NewsAPI, and Bloomberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29d4a3",
   "metadata": {},
   "source": [
    "**Load Data (Yahoo Finance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ca684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKE: fetched 10 raw items\n",
      "LULU: fetched 10 raw items\n",
      "ATZ.TO: fetched 10 raw items\n",
      "                                       id  \\\n",
      "0  4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60-0   \n",
      "1  44127e76-4d51-3341-91f7-75a151c327de-0   \n",
      "2  d27b1692-49b5-37f9-8f7f-170607a64bc6-0   \n",
      "3  adf56cf4-fc14-38e2-8b57-4bdc4485cefa-0   \n",
      "4  45fdbd44-5798-37fc-9b78-dbf1bb62b6f0-0   \n",
      "\n",
      "                              report_id  ticker company  \\\n",
      "0  4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60  ATZ.TO           \n",
      "1  44127e76-4d51-3341-91f7-75a151c327de  ATZ.TO           \n",
      "2  d27b1692-49b5-37f9-8f7f-170607a64bc6  ATZ.TO           \n",
      "3  adf56cf4-fc14-38e2-8b57-4bdc4485cefa  ATZ.TO           \n",
      "4  45fdbd44-5798-37fc-9b78-dbf1bb62b6f0  ATZ.TO           \n",
      "\n",
      "                       date    source doc_type item section_type  \\\n",
      "0 2025-12-22 13:58:54+00:00  yfinance    STORY              news   \n",
      "1 2025-12-22 12:38:14+00:00  yfinance    STORY              news   \n",
      "2 2025-12-19 19:23:11+00:00  yfinance    STORY              news   \n",
      "3 2025-12-12 12:35:50+00:00  yfinance    STORY              news   \n",
      "4 2025-12-05 17:49:06+00:00  yfinance    STORY              news   \n",
      "\n",
      "                                     section_heading  chunk_index page_start  \\\n",
      "0  The investing winners and losers that made or ...            0              \n",
      "1  TSX Value Picks Including Aritzia And Two Othe...            0              \n",
      "2  Stifel Canada Names Gildan, KITS, and Couche-T...            0              \n",
      "3  3 TSX Growth Stocks With Up To 22% Insider Own...            0              \n",
      "4  Tech Tactics: Aritzia Taps Nedapâ€™s RFID Platfo...            0              \n",
      "\n",
      "  page_end                                               text  \\\n",
      "0           Despite turmoil from the trade war, most globa...   \n",
      "1           As 2025 draws to a close, the Canadian market ...   \n",
      "2           Stifel Canada said Friday its best ideas for C...   \n",
      "3           As we approach the end of 2025, Canadian marke...   \n",
      "4           Aritzia utilizes Nedap's RFID platform to stre...   \n",
      "\n",
      "                                         source_file  \n",
      "0  https://ca.finance.yahoo.com/news/were-investi...  \n",
      "1  https://finance.yahoo.com/news/tsx-value-picks...  \n",
      "2  https://finance.yahoo.com/news/stifel-canada-n...  \n",
      "3  https://finance.yahoo.com/news/3-tsx-growth-st...  \n",
      "4  https://sourcingjournal.com/topics/technology/...  \n",
      "\n",
      "                                            id  \\\n",
      "count                                       30   \n",
      "unique                                      30   \n",
      "top     4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60-0   \n",
      "freq                                         1   \n",
      "mean                                       NaN   \n",
      "min                                        NaN   \n",
      "25%                                        NaN   \n",
      "50%                                        NaN   \n",
      "75%                                        NaN   \n",
      "max                                        NaN   \n",
      "std                                        NaN   \n",
      "\n",
      "                                   report_id  ticker company  \\\n",
      "count                                     30      30      30   \n",
      "unique                                    30       3       1   \n",
      "top     4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60  ATZ.TO           \n",
      "freq                                       1      10      30   \n",
      "mean                                     NaN     NaN     NaN   \n",
      "min                                      NaN     NaN     NaN   \n",
      "25%                                      NaN     NaN     NaN   \n",
      "50%                                      NaN     NaN     NaN   \n",
      "75%                                      NaN     NaN     NaN   \n",
      "max                                      NaN     NaN     NaN   \n",
      "std                                      NaN     NaN     NaN   \n",
      "\n",
      "                                       date    source doc_type item  \\\n",
      "count                                    30        30       30   30   \n",
      "unique                                  NaN         1        1    1   \n",
      "top                                     NaN  yfinance    STORY        \n",
      "freq                                    NaN        30       30   30   \n",
      "mean    2025-12-19 20:31:13.533333504+00:00       NaN      NaN  NaN   \n",
      "min               2025-11-19 12:38:06+00:00       NaN      NaN  NaN   \n",
      "25%     2025-12-20 11:27:08.249999872+00:00       NaN      NaN  NaN   \n",
      "50%               2025-12-24 11:28:30+00:00       NaN      NaN  NaN   \n",
      "75%               2025-12-26 18:51:00+00:00       NaN      NaN  NaN   \n",
      "max               2025-12-29 01:20:00+00:00       NaN      NaN  NaN   \n",
      "std                                     NaN       NaN      NaN  NaN   \n",
      "\n",
      "       section_type                                    section_heading  \\\n",
      "count            30                                                 30   \n",
      "unique            1                                                 30   \n",
      "top            news  The investing winners and losers that made or ...   \n",
      "freq             30                                                  1   \n",
      "mean            NaN                                                NaN   \n",
      "min             NaN                                                NaN   \n",
      "25%             NaN                                                NaN   \n",
      "50%             NaN                                                NaN   \n",
      "75%             NaN                                                NaN   \n",
      "max             NaN                                                NaN   \n",
      "std             NaN                                                NaN   \n",
      "\n",
      "        chunk_index page_start page_end  \\\n",
      "count          30.0         30       30   \n",
      "unique          NaN          1        1   \n",
      "top             NaN                       \n",
      "freq            NaN         30       30   \n",
      "mean            0.0        NaN      NaN   \n",
      "min             0.0        NaN      NaN   \n",
      "25%             0.0        NaN      NaN   \n",
      "50%             0.0        NaN      NaN   \n",
      "75%             0.0        NaN      NaN   \n",
      "max             0.0        NaN      NaN   \n",
      "std             0.0        NaN      NaN   \n",
      "\n",
      "                                                     text  \\\n",
      "count                                                  30   \n",
      "unique                                                 30   \n",
      "top     Despite turmoil from the trade war, most globa...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "std                                                   NaN   \n",
      "\n",
      "                                              source_file  \n",
      "count                                                  30  \n",
      "unique                                                 30  \n",
      "top     https://ca.finance.yahoo.com/news/were-investi...  \n",
      "freq                                                    1  \n",
      "mean                                                  NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "std                                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, UTC\n",
    "import time\n",
    "\n",
    "CANONICAL_FIELDS = [\n",
    "    \"id\",\n",
    "    \"report_id\",\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"date\",\n",
    "    \"source\",\n",
    "    \"doc_type\",\n",
    "    \"item\",\n",
    "    \"section_type\",\n",
    "    \"section_heading\",\n",
    "    \"chunk_index\",\n",
    "    \"page_start\",\n",
    "    \"page_end\",\n",
    "    \"text\",\n",
    "    \"source_file\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_row(*, report_id, text, chunk_index, source, source_file, ticker=None, company=None, date=None, doc_type=None, item=None, section_type=None, section_heading=None, page_start=None, page_end=None):\n",
    "    return {\n",
    "        \"id\": f\"{report_id}-{chunk_index}\",\n",
    "        \"report_id\": report_id,\n",
    "        \"ticker\": ticker or \"\",\n",
    "        \"company\": company or \"\",\n",
    "        \"date\": date or \"\",\n",
    "        \"source\": source,\n",
    "        \"doc_type\": doc_type or \"\",\n",
    "        \"item\": item or \"\",\n",
    "        \"section_type\": section_type or \"\",\n",
    "        \"section_heading\": section_heading or \"\",\n",
    "        \"chunk_index\": chunk_index,\n",
    "        \"page_start\": page_start or \"\",\n",
    "        \"page_end\": page_end or \"\",\n",
    "        \"text\": (text or \"\").strip(),\n",
    "        \"source_file\": source_file,\n",
    "    }\n",
    "\n",
    "# Tickers to analyze\n",
    "TICKERS = [\"NKE\", \"LULU\", \"ATZ.TO\"]\n",
    "YF_MAX_ITEMS = 10  # yfinance often returns ~10\n",
    "LOOKBACK_DAYS = 365\n",
    "\n",
    "\n",
    "def extract_article(item: dict, ticker: str) -> dict:\n",
    "    content = item.get(\"content\", {}) or {}\n",
    "\n",
    "    # Published time: providerPublishTime (unix) or content pubDate/displayTime\n",
    "    published_dt = None\n",
    "    ts = item.get(\"providerPublishTime\")\n",
    "    if ts:\n",
    "        try:\n",
    "            published_dt = datetime.fromtimestamp(ts, tz=UTC)\n",
    "        except Exception:\n",
    "            published_dt = None\n",
    "    if published_dt is None:\n",
    "        pub_iso = content.get(\"pubDate\") or content.get(\"displayTime\")\n",
    "        if pub_iso:\n",
    "            published_dt = pd.to_datetime(pub_iso, errors=\"coerce\", utc=True)\n",
    "\n",
    "    link = (\n",
    "        item.get(\"link\")\n",
    "        or (item.get(\"canonicalUrl\") or {}).get(\"url\")\n",
    "        or (item.get(\"clickThroughUrl\") or {}).get(\"url\")\n",
    "        or (content.get(\"canonicalUrl\") or {}).get(\"url\")\n",
    "        or (content.get(\"clickThroughUrl\") or {}).get(\"url\")\n",
    "    )\n",
    "    report_id = item.get(\"id\") or item.get(\"uuid\") or f\"{ticker}-{int(time.time())}\"\n",
    "    publisher = (item.get(\"publisher\") or (content.get(\"provider\") or {}).get(\"displayName\") or \"\").strip()\n",
    "    doc_type = (item.get(\"type\") or content.get(\"contentType\") or \"news\").strip()\n",
    "    heading = (content.get(\"title\") or item.get(\"title\") or \"\").strip()\n",
    "    text = (content.get(\"summary\") or content.get(\"description\") or item.get(\"summary\") or heading).strip()\n",
    "\n",
    "    return make_row(\n",
    "        report_id=report_id,\n",
    "        text=text,\n",
    "        chunk_index=0,\n",
    "        source=\"yfinance\",\n",
    "        source_file=link or \"\",\n",
    "        ticker=ticker,\n",
    "        company=\"\",\n",
    "        date=str(published_dt) if published_dt is not None else \"\",\n",
    "        doc_type=doc_type,\n",
    "        section_type=\"news\",\n",
    "        section_heading=heading,\n",
    "        page_start=\"\",\n",
    "        page_end=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "rows = []\n",
    "for tic in TICKERS:\n",
    "    raw_news = (yf.Ticker(tic).news or [])[:YF_MAX_ITEMS]\n",
    "    rows.extend([extract_article(item, tic) for item in raw_news])\n",
    "    print(f\"{tic}: fetched {len(raw_news)} raw items\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Build dataframe\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.reindex(columns=CANONICAL_FIELDS)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=LOOKBACK_DAYS)\n",
    "    df = df[df[\"date\"].isna() | (df[\"date\"] >= cutoff)]\n",
    "    df = df.drop_duplicates(subset=[\"section_heading\", \"source_file\"])\n",
    "    df = df.sort_values([\"ticker\", \"date\"], ascending=[True, False]).reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=CANONICAL_FIELDS)\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72945253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to ./processed_data/financial_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = \"./processed_data/financial_news_dataset.csv\"  # optional export\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Data exported to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa9a25",
   "metadata": {},
   "source": [
    "**Import Data From News API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2bfb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "keywords = {\n",
    "    \"nike\": [\"nke\",\"nike\",\"Nike\",\"NIKE\",\"NKE\"],\n",
    "    \"atz\": [\"ATZ\", \"atz\", \"Aritzia\",\"aritzia\"],\n",
    "    \"lulu\": [\"lulu\", \"LULU\",\"lululemon\",\"Lululemon\"]\n",
    "}\n",
    "\n",
    "DATE = '2025-12-01'\n",
    "\n",
    "api_key= os.getenv('NEWS_API_KEY')\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcef422",
   "metadata": {},
   "source": [
    "**Export NewsAPI articles to CSV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 228 rows to processed_data/newsapi_articles_2.csv\n",
      "                                   id                         report_id  \\\n",
      "0  901f62935ab1ec782941f34fd56e02c5-0  901f62935ab1ec782941f34fd56e02c5   \n",
      "1  0730dad345991302372aad0c2bc823b8-0  0730dad345991302372aad0c2bc823b8   \n",
      "2  09dec5d562f4ad698bbddf97d919b036-0  09dec5d562f4ad698bbddf97d919b036   \n",
      "3  f2a7a2077a29657aef0b8ffb860f1c2b-0  f2a7a2077a29657aef0b8ffb860f1c2b   \n",
      "4  c85829c8e35c4653935d1b79c15d3785-0  c85829c8e35c4653935d1b79c15d3785   \n",
      "\n",
      "  ticker company                  date   source doc_type item section_type  \\\n",
      "0    NKE    nike  2025-12-24T16:41:25Z  newsapi     news              news   \n",
      "1    NKE    nike  2025-12-10T14:58:59Z  newsapi     news              news   \n",
      "2    NKE    nike  2025-12-25T10:26:45Z  newsapi     news              news   \n",
      "3    NKE    nike  2025-12-19T15:49:51Z  newsapi     news              news   \n",
      "4    NKE    nike  2025-12-10T15:30:22Z  newsapi     news              news   \n",
      "\n",
      "                                     section_heading  chunk_index page_start  \\\n",
      "0  Apple CEO Tim Cook Buys $3 Million of Nike Shares            0              \n",
      "1  Meet Gen Z's latest obsession: A sneaker so bo...            0              \n",
      "2  Tim Cook just gave Nike a much-needed holiday ...            0              \n",
      "3  Nike is struggling to stay culturally relevant...            0              \n",
      "4  WTA signs with Mercedes in 'most significant d...            0              \n",
      "\n",
      "  page_end                                               text  \\\n",
      "0           Apple CEO Tim Cook disclosed a roughly $3 mill...   \n",
      "1           Asics; Hoka; Tyler Le/BI\\r\\n<ul><li>A Strava r...   \n",
      "2           Tim Cook sporting a custom pair of Nike Vomero...   \n",
      "3           Pedestrians walk past a Nike store in China.Ch...   \n",
      "4           The Women's Tennis Association has announced a...   \n",
      "\n",
      "                                         source_file  \n",
      "0  https://www.macrumors.com/2025/12/24/tim-cook-...  \n",
      "1  https://www.businessinsider.com/gen-z-runners-...  \n",
      "2  https://www.businessinsider.com/nike-tim-cook-...  \n",
      "3  https://www.businessinsider.com/nike-sales-dec...  \n",
      "4  https://www.bbc.com/sport/tennis/articles/c14v...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "CANONICAL_FIELDS = [\n",
    "    \"id\",\n",
    "    \"report_id\",\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"date\",\n",
    "    \"source\",\n",
    "    \"doc_type\",\n",
    "    \"item\",\n",
    "    \"section_type\",\n",
    "    \"section_heading\",\n",
    "    \"chunk_index\",\n",
    "    \"page_start\",\n",
    "    \"page_end\",\n",
    "    \"text\",\n",
    "    \"source_file\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_row(*, report_id, text, chunk_index, source, source_file, ticker=None, company=None, date=None, doc_type=None, item=None, section_type=None, section_heading=None, page_start=None, page_end=None):\n",
    "    return {\n",
    "        \"id\": f\"{report_id}-{chunk_index}\",\n",
    "        \"report_id\": report_id,\n",
    "        \"ticker\": ticker or \"\",\n",
    "        \"company\": company or \"\",\n",
    "        \"date\": date or \"\",\n",
    "        \"source\": source,\n",
    "        \"doc_type\": doc_type or \"\",\n",
    "        \"item\": item or \"\",\n",
    "        \"section_type\": section_type or \"\",\n",
    "        \"section_heading\": section_heading or \"\",\n",
    "        \"chunk_index\": chunk_index,\n",
    "        \"page_start\": page_start or \"\",\n",
    "        \"page_end\": page_end or \"\",\n",
    "        \"text\": (text or \"\").strip(),\n",
    "        \"source_file\": source_file,\n",
    "    }\n",
    "\n",
    "\n",
    "def stable_id(text: str, url: str) -> str:\n",
    "    basis = (url or text or \"\").encode(\"utf-8\")\n",
    "    return hashlib.md5(basis, usedforsecurity=False).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_news_articles(keywords, start_date):\n",
    "    rows = []\n",
    "    for kw, words in keywords.items():\n",
    "        query = \" OR \".join(words)\n",
    "        resp = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=start_date,\n",
    "            language=\"en\",\n",
    "            sort_by=\"relevancy\",\n",
    "            page=1,\n",
    "            page_size=100,\n",
    "        )\n",
    "        ticker = \"\"\n",
    "        if kw == \"nike\":\n",
    "            ticker = \"NKE\"\n",
    "        if kw == \"atz\":\n",
    "            ticker = \"ATZ\"\n",
    "        if kw == \"lulu\":\n",
    "            ticker = \"LULU\"\n",
    "\n",
    "        for art in resp.get(\"articles\", []):\n",
    "            title = art.get(\"title\") or \"\"\n",
    "            text = (art.get(\"content\") or art.get(\"description\") or title).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            url = art.get(\"url\") or \"\"\n",
    "            rid = stable_id(title, url)\n",
    "            rows.append(\n",
    "                make_row(\n",
    "                    report_id=rid,\n",
    "                    text=text,\n",
    "                    chunk_index=0,\n",
    "                    source=\"newsapi\",\n",
    "                    source_file=url,\n",
    "                    ticker=ticker,\n",
    "                    company=kw,\n",
    "                    date=art.get(\"publishedAt\") or \"\",\n",
    "                    doc_type=\"news\",\n",
    "                    section_type=\"news\",\n",
    "                    section_heading=title,\n",
    "                )\n",
    "            )\n",
    "    return pd.DataFrame(rows, columns=CANONICAL_FIELDS)\n",
    "\n",
    "\n",
    "df_news = fetch_news_articles(keywords, DATE)\n",
    "\n",
    "Path(\"processed_data\").mkdir(parents=True, exist_ok=True)\n",
    "output_path = \"processed_data/newsapi_articles.csv\"\n",
    "df_news.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df_news)} rows to {output_path}\")\n",
    "print(df_news.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375246a",
   "metadata": {},
   "source": [
    "**Analyzing Equity Research Reports from Bloomberg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36586d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDone in pdf_section_extractor.py\\n\\nPulls thesis/growth/risk/valuation/earnings blocks using simple\\nheading heuristics, then optionally chunks text for BERT-friendly input.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Done in pdf_section_extractor.py\n",
    "\n",
    "Pulls thesis/growth/risk/valuation/earnings blocks using simple\n",
    "heading heuristics, then optionally chunks text for BERT-friendly input.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff4a87",
   "metadata": {},
   "source": [
    "**SEC + TSEC Filings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6616f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skurono\\Desktop\\SCMC\\scmc-2025\\venv\\Lib\\site-packages\\secedgar\\client.py:218: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  return BeautifulSoup(self.get_response(path, params, **kwargs).text,\n"
     ]
    }
   ],
   "source": [
    "from secedgar import filings, FilingType\n",
    "\n",
    "\"\"\"\n",
    "# 8K filings for Nike and Lululemon (tickers \"nke\" and \"lulu\")\n",
    "my_filings_8k = filings(cik_lookup=[\"nke\",\"lulu\"],\n",
    "                     filing_type=FilingType.FILING_8K,\n",
    "                     user_agent=\"Simon Kurono (simonkurono@gmail.com)\")\n",
    "\n",
    "my_filings_8k.save('./nlp/raw_data/sec_filings_8k')\n",
    "\n",
    "# 10Q filings for Nike and Lululemon (tickers \"nke\" and \"lulu\")\n",
    "my_filings_8k = filings(cik_lookup=[\"nke\",\"lulu\"],\n",
    "                     filing_type=FilingType.FILING_10Q,\n",
    "                     user_agent=\"Simon Kurono (simonkurono@gmail.com)\")\n",
    "\n",
    "my_filings_8k.save('./nlp/raw_data/sec_filings_10q')\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
