{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f840f",
   "metadata": {},
   "source": [
    "## Load Data from Yfinance, NewsAPI, and Bloomberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29d4a3",
   "metadata": {},
   "source": [
    "**Load Data (Yahoo Finance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKE: fetched 10 raw items\n",
      "LULU: fetched 10 raw items\n",
      "ATZ: fetched 0 raw items\n",
      "                                       id  \\\n",
      "0  a9c01356-9d78-3d3f-8132-f9ac193397fb-0   \n",
      "1  93b034dd-79fd-37df-9752-a3ea786023d0-0   \n",
      "2  eed1f940-1dba-315b-8ed9-63073c56725b-0   \n",
      "3  27d1d31f-05f4-3fe3-b142-8cdcce89dd8e-0   \n",
      "4  12cc9c8f-c138-3d15-8795-91991bd0cbd1-0   \n",
      "\n",
      "                              report_id ticker company  \\\n",
      "0  a9c01356-9d78-3d3f-8132-f9ac193397fb   LULU           \n",
      "1  93b034dd-79fd-37df-9752-a3ea786023d0   LULU           \n",
      "2  eed1f940-1dba-315b-8ed9-63073c56725b   LULU           \n",
      "3  27d1d31f-05f4-3fe3-b142-8cdcce89dd8e   LULU           \n",
      "4  12cc9c8f-c138-3d15-8795-91991bd0cbd1   LULU           \n",
      "\n",
      "                       date    source doc_type item section_type  \\\n",
      "0 2025-12-30 00:39:00+00:00  yfinance    STORY              news   \n",
      "1 2025-12-29 21:46:44+00:00  yfinance    STORY              news   \n",
      "2 2025-12-29 21:44:16+00:00  yfinance    VIDEO              news   \n",
      "3 2025-12-29 20:56:15+00:00  yfinance    STORY              news   \n",
      "4 2025-12-29 20:56:00+00:00  yfinance    STORY              news   \n",
      "\n",
      "                                     section_heading  chunk_index page_start  \\\n",
      "0  Lululemon Founder Chip Wilson Launches Proxy F...            0              \n",
      "1  Stocks to Watch Monday Recap: Lululemon, Newmo...            0              \n",
      "2             Lululemon founder launches proxy fight            0              \n",
      "3  Sector Update: Consumer Stocks Mixed Late Afte...            0              \n",
      "4  Lululemon Founder Chip Wilson Launches Proxy B...            0              \n",
      "\n",
      "  page_end                                               text  \\\n",
      "0           The founder‚Äôs proposal seeks to make boardroom...   \n",
      "1           üîé Lululemon (LULU): The sports-clothes maker's...   \n",
      "2           Lululemon (LULU) shares closed 1.7% higher on ...   \n",
      "3           Consumer stocks were mixed late Monday afterno...   \n",
      "4           Lululemon  Athletica‚Äôs founder Chip Wilson is ...   \n",
      "\n",
      "                                         source_file  \n",
      "0  https://www.wsj.com/business/lululemon-activis...  \n",
      "1  https://www.wsj.com/livecoverage/stock-market-...  \n",
      "2  https://finance.yahoo.com/video/lululemon-foun...  \n",
      "3  https://finance.yahoo.com/news/sector-consumer...  \n",
      "4  https://www.barrons.com/articles/lululemon-sto...  \n",
      "\n",
      "                                            id  \\\n",
      "count                                       20   \n",
      "unique                                      20   \n",
      "top     a9c01356-9d78-3d3f-8132-f9ac193397fb-0   \n",
      "freq                                         1   \n",
      "mean                                       NaN   \n",
      "min                                        NaN   \n",
      "25%                                        NaN   \n",
      "50%                                        NaN   \n",
      "75%                                        NaN   \n",
      "max                                        NaN   \n",
      "std                                        NaN   \n",
      "\n",
      "                                   report_id ticker company  \\\n",
      "count                                     20     20      20   \n",
      "unique                                    20      2       1   \n",
      "top     a9c01356-9d78-3d3f-8132-f9ac193397fb   LULU           \n",
      "freq                                       1     10      20   \n",
      "mean                                     NaN    NaN     NaN   \n",
      "min                                      NaN    NaN     NaN   \n",
      "25%                                      NaN    NaN     NaN   \n",
      "50%                                      NaN    NaN     NaN   \n",
      "75%                                      NaN    NaN     NaN   \n",
      "max                                      NaN    NaN     NaN   \n",
      "std                                      NaN    NaN     NaN   \n",
      "\n",
      "                                       date    source doc_type item  \\\n",
      "count                                    20        20       20   20   \n",
      "unique                                  NaN         1        2    1   \n",
      "top                                     NaN  yfinance    STORY        \n",
      "freq                                    NaN        20       18   20   \n",
      "mean    2025-12-29 04:50:06.349999872+00:00       NaN      NaN  NaN   \n",
      "min               2025-12-26 17:42:00+00:00       NaN      NaN  NaN   \n",
      "25%     2025-12-28 22:59:44.750000128+00:00       NaN      NaN  NaN   \n",
      "50%        2025-12-29 18:27:50.500000+00:00       NaN      NaN  NaN   \n",
      "75%     2025-12-29 20:56:03.750000128+00:00       NaN      NaN  NaN   \n",
      "max               2025-12-30 00:39:00+00:00       NaN      NaN  NaN   \n",
      "std                                     NaN       NaN      NaN  NaN   \n",
      "\n",
      "       section_type                                    section_heading  \\\n",
      "count            20                                                 20   \n",
      "unique            1                                                 20   \n",
      "top            news  Lululemon Founder Chip Wilson Launches Proxy F...   \n",
      "freq             20                                                  1   \n",
      "mean            NaN                                                NaN   \n",
      "min             NaN                                                NaN   \n",
      "25%             NaN                                                NaN   \n",
      "50%             NaN                                                NaN   \n",
      "75%             NaN                                                NaN   \n",
      "max             NaN                                                NaN   \n",
      "std             NaN                                                NaN   \n",
      "\n",
      "        chunk_index page_start page_end  \\\n",
      "count          20.0         20       20   \n",
      "unique          NaN          1        1   \n",
      "top             NaN                       \n",
      "freq            NaN         20       20   \n",
      "mean            0.0        NaN      NaN   \n",
      "min             0.0        NaN      NaN   \n",
      "25%             0.0        NaN      NaN   \n",
      "50%             0.0        NaN      NaN   \n",
      "75%             0.0        NaN      NaN   \n",
      "max             0.0        NaN      NaN   \n",
      "std             0.0        NaN      NaN   \n",
      "\n",
      "                                                     text  \\\n",
      "count                                                  20   \n",
      "unique                                                 20   \n",
      "top     The founder‚Äôs proposal seeks to make boardroom...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "std                                                   NaN   \n",
      "\n",
      "                                              source_file  \n",
      "count                                                  20  \n",
      "unique                                                 20  \n",
      "top     https://www.wsj.com/business/lululemon-activis...  \n",
      "freq                                                    1  \n",
      "mean                                                  NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "std                                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, UTC\n",
    "import time\n",
    "\n",
    "CANONICAL_FIELDS = [\n",
    "    \"id\",\n",
    "    \"report_id\",\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"date\",\n",
    "    \"source\",\n",
    "    \"doc_type\",\n",
    "    \"item\",\n",
    "    \"section_type\",\n",
    "    \"section_heading\",\n",
    "    \"chunk_index\",\n",
    "    \"page_start\",\n",
    "    \"page_end\",\n",
    "    \"text\",\n",
    "    \"source_file\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_row(*, report_id, text, chunk_index, source, source_file, ticker=None, company=None, date=None, doc_type=None, item=None, section_type=None, section_heading=None, page_start=None, page_end=None):\n",
    "    return {\n",
    "        \"id\": f\"{report_id}-{chunk_index}\",\n",
    "        \"report_id\": report_id,\n",
    "        \"ticker\": ticker or \"\",\n",
    "        \"company\": company or \"\",\n",
    "        \"date\": date or \"\",\n",
    "        \"source\": source,\n",
    "        \"doc_type\": doc_type or \"\",\n",
    "        \"item\": item or \"\",\n",
    "        \"section_type\": section_type or \"\",\n",
    "        \"section_heading\": section_heading or \"\",\n",
    "        \"chunk_index\": chunk_index,\n",
    "        \"page_start\": page_start or \"\",\n",
    "        \"page_end\": page_end or \"\",\n",
    "        \"text\": (text or \"\").strip(),\n",
    "        \"source_file\": source_file,\n",
    "    }\n",
    "\n",
    "# Tickers to analyze\n",
    "TICKERS = [\"NKE\", \"LULU\", \"ATZ.CO\"]\n",
    "YF_MAX_ITEMS = 10  # yfinance often returns ~10\n",
    "LOOKBACK_DAYS = 365\n",
    "\n",
    "\n",
    "def extract_article(item: dict, ticker: str) -> dict:\n",
    "    content = item.get(\"content\", {}) or {}\n",
    "\n",
    "    # Published time: providerPublishTime (unix) or content pubDate/displayTime\n",
    "    published_dt = None\n",
    "    ts = item.get(\"providerPublishTime\")\n",
    "    if ts:\n",
    "        try:\n",
    "            published_dt = datetime.fromtimestamp(ts, tz=UTC)\n",
    "        except Exception:\n",
    "            published_dt = None\n",
    "    if published_dt is None:\n",
    "        pub_iso = content.get(\"pubDate\") or content.get(\"displayTime\")\n",
    "        if pub_iso:\n",
    "            published_dt = pd.to_datetime(pub_iso, errors=\"coerce\", utc=True)\n",
    "\n",
    "    link = (\n",
    "        item.get(\"link\")\n",
    "        or (item.get(\"canonicalUrl\") or {}).get(\"url\")\n",
    "        or (item.get(\"clickThroughUrl\") or {}).get(\"url\")\n",
    "        or (content.get(\"canonicalUrl\") or {}).get(\"url\")\n",
    "        or (content.get(\"clickThroughUrl\") or {}).get(\"url\")\n",
    "    )\n",
    "    report_id = item.get(\"id\") or item.get(\"uuid\") or f\"{ticker}-{int(time.time())}\"\n",
    "    publisher = (item.get(\"publisher\") or (content.get(\"provider\") or {}).get(\"displayName\") or \"\").strip()\n",
    "    doc_type = (item.get(\"type\") or content.get(\"contentType\") or \"news\").strip()\n",
    "    heading = (content.get(\"title\") or item.get(\"title\") or \"\").strip()\n",
    "    text = (content.get(\"summary\") or content.get(\"description\") or item.get(\"summary\") or heading).strip()\n",
    "\n",
    "    return make_row(\n",
    "        report_id=report_id,\n",
    "        text=text,\n",
    "        chunk_index=0,\n",
    "        source=\"yfinance\",\n",
    "        source_file=link or \"\",\n",
    "        ticker=ticker,\n",
    "        company=\"\",\n",
    "        date=str(published_dt) if published_dt is not None else \"\",\n",
    "        doc_type=doc_type,\n",
    "        section_type=\"news\",\n",
    "        section_heading=heading,\n",
    "        page_start=\"\",\n",
    "        page_end=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "rows = []\n",
    "for tic in TICKERS:\n",
    "    raw_news = (yf.Ticker(tic).news or [])[:YF_MAX_ITEMS]\n",
    "    rows.extend([extract_article(item, tic) for item in raw_news])\n",
    "    print(f\"{tic}: fetched {len(raw_news)} raw items\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Build dataframe\n",
    "if rows:\n",
    "    master = pd.DataFrame(rows)\n",
    "    master = master.reindex(columns=CANONICAL_FIELDS)\n",
    "    master[\"date\"] = pd.to_datetime(master[\"date\"], errors=\"coerce\", utc=True)\n",
    "    cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=LOOKBACK_DAYS)\n",
    "    master = master[master[\"date\"].isna() | (master[\"date\"] >= cutoff)]\n",
    "    master = master.drop_duplicates(subset=[\"section_heading\", \"source_file\"])\n",
    "    master = master.sort_values([\"ticker\", \"date\"], ascending=[True, False]).reset_index(drop=True)\n",
    "else:\n",
    "    master = pd.DataFrame(columns=CANONICAL_FIELDS)\n",
    "\n",
    "print(master.head())\n",
    "print()\n",
    "print(master.describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72945253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to ./processed_data/financial_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = \"./processed_data/final/financial_news_dataset.csv\"  # optional export\n",
    "master.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Data exported to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa9a25",
   "metadata": {},
   "source": [
    "**Import Data From News API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2bfb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "TICKERS = [\"NKE\", \"LULU\", \"ATZ.TO\"]\n",
    "keywords = {\n",
    "    #\"nike\": [\"nke\",\"NKE\"],\n",
    "    \"atz\": [\"ATZ\", \"atz\", \"atz.co\",\"ATZ.CO, ATZ.TO\", \"atz.to\", \"Aritzia\"]\n",
    "    #\"lulu\": [\"lulu\", \"LULU\"]\n",
    "}\n",
    "\n",
    "DATE = '2025-12-01'\n",
    "\n",
    "api_key= os.getenv('NEWS_API_KEY')\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcef422",
   "metadata": {},
   "source": [
    "**Export NewsAPI articles to CSV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1686f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 32 rows to processed_data/newsapi_articles.csv\n",
      "                                   id                         report_id  \\\n",
      "0  5f922d2adea94483311ca54f04ba73b9-0  5f922d2adea94483311ca54f04ba73b9   \n",
      "1  85d06731fb1c9c177393344a45b7e707-0  85d06731fb1c9c177393344a45b7e707   \n",
      "2  ee8da6e3d769ca0df97ba314940fb9a5-0  ee8da6e3d769ca0df97ba314940fb9a5   \n",
      "3  7ec1e9450737c4ad4481ec624aee54b0-0  7ec1e9450737c4ad4481ec624aee54b0   \n",
      "4  0fed61e7630571098a8d4ec50d6cc30d-0  0fed61e7630571098a8d4ec50d6cc30d   \n",
      "\n",
      "  ticker company                  date   source doc_type item section_type  \\\n",
      "0    ATZ     atz  2025-12-20T10:55:01Z  newsapi     news              news   \n",
      "1    ATZ     atz  2025-12-05T17:49:06Z  newsapi     news              news   \n",
      "2    ATZ     atz  2025-12-10T13:00:45Z  newsapi     news              news   \n",
      "3    ATZ     atz  2025-12-12T13:38:01Z  newsapi     news              news   \n",
      "4    ATZ     atz  2025-12-23T18:30:00Z  newsapi     news              news   \n",
      "\n",
      "                                     section_heading  chunk_index page_start  \\\n",
      "0  I went to see the hype about Aritzia's Super P...            0              \n",
      "1  Tech Tactics: Aritzia Taps Nedap‚Äôs RFID Platfo...            0              \n",
      "2  Gift of the Day: The World‚Äôs Most Versatile Ca...            0              \n",
      "3  The best street-style outfits Taylor Swift has...            0              \n",
      "4  The Workout Sets We‚Äôll Be Wearing on Repeat in...            0              \n",
      "\n",
      "  page_end                                               text  \\\n",
      "0           The modern taupe liquid shine Super Puff was a...   \n",
      "1           Tech Tactics is Sourcing Journals series with ...   \n",
      "2           The best holiday gifts are the ones that are v...   \n",
      "3           Taylor Swift in New York City on December 10, ...   \n",
      "4           If youre anything like us, you might be planni...   \n",
      "\n",
      "                                         source_file  \n",
      "0  https://www.businessinsider.com/aritzia-super-...  \n",
      "1  https://sourcingjournal.com/topics/technology/...  \n",
      "2  http://www.thecut.com/article/gift-of-the-day-...  \n",
      "3  https://www.businessinsider.com/taylor-swift-s...  \n",
      "4  https://www.eonline.com/news/1426523/cute-work...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "CANONICAL_FIELDS = [\n",
    "    \"id\",\n",
    "    \"report_id\",\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"date\",\n",
    "    \"source\",\n",
    "    \"doc_type\",\n",
    "    \"item\",\n",
    "    \"section_type\",\n",
    "    \"section_heading\",\n",
    "    \"chunk_index\",\n",
    "    \"page_start\",\n",
    "    \"page_end\",\n",
    "    \"text\",\n",
    "    \"source_file\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_row(*, report_id, text, chunk_index, source, source_file, ticker=None, company=None, date=None, doc_type=None, item=None, section_type=None, section_heading=None, page_start=None, page_end=None):\n",
    "    return {\n",
    "        \"id\": f\"{report_id}-{chunk_index}\",\n",
    "        \"report_id\": report_id,\n",
    "        \"ticker\": ticker or \"\",\n",
    "        \"company\": company or \"\",\n",
    "        \"date\": date or \"\",\n",
    "        \"source\": source,\n",
    "        \"doc_type\": doc_type or \"\",\n",
    "        \"item\": item or \"\",\n",
    "        \"section_type\": section_type or \"\",\n",
    "        \"section_heading\": section_heading or \"\",\n",
    "        \"chunk_index\": chunk_index,\n",
    "        \"page_start\": page_start or \"\",\n",
    "        \"page_end\": page_end or \"\",\n",
    "        \"text\": (text or \"\").strip(),\n",
    "        \"source_file\": source_file,\n",
    "    }\n",
    "\n",
    "\n",
    "def stable_id(text: str, url: str) -> str:\n",
    "    basis = (url or text or \"\").encode(\"utf-8\")\n",
    "    return hashlib.md5(basis, usedforsecurity=False).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_news_articles(keywords, start_date):\n",
    "    rows = []\n",
    "    for kw, words in keywords.items():\n",
    "        query = \" OR \".join(words)\n",
    "        resp = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=start_date,\n",
    "            language=\"en\",\n",
    "            sort_by=\"relevancy\",\n",
    "            page=1,\n",
    "            page_size=100,\n",
    "        )\n",
    "        ticker = \"\"\n",
    "        if kw == \"nike\":\n",
    "            ticker = \"NKE\"\n",
    "        if kw == \"atz\":\n",
    "            ticker = \"ATZ\"\n",
    "        if kw == \"lulu\":\n",
    "            ticker = \"LULU\"\n",
    "\n",
    "        for art in resp.get(\"articles\", []):\n",
    "            title = art.get(\"title\") or \"\"\n",
    "            text = (art.get(\"content\") or art.get(\"description\") or title).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            url = art.get(\"url\") or \"\"\n",
    "            rid = stable_id(title, url)\n",
    "            rows.append(\n",
    "                make_row(\n",
    "                    report_id=rid,\n",
    "                    text=text,\n",
    "                    chunk_index=0,\n",
    "                    source=\"newsapi\",\n",
    "                    source_file=url,\n",
    "                    ticker=ticker,\n",
    "                    company=kw,\n",
    "                    date=art.get(\"publishedAt\") or \"\",\n",
    "                    doc_type=\"news\",\n",
    "                    section_type=\"news\",\n",
    "                    section_heading=title,\n",
    "                )\n",
    "            )\n",
    "    return pd.DataFrame(rows, columns=CANONICAL_FIELDS)\n",
    "\n",
    "\n",
    "df_news = fetch_news_articles(keywords, DATE)\n",
    "\n",
    "output_path = \"processed_data/newsapi_articles.csv\"\n",
    "df_news.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df_news)} rows to {output_path}\")\n",
    "print(df_news.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375246a",
   "metadata": {},
   "source": [
    "**Analyzing Equity Research Reports from Bloomberg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36586d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDone in pdf_section_extractor.py\\n\\nPulls thesis/growth/risk/valuation/earnings blocks using simple\\nheading heuristics, then optionally chunks text for BERT-friendly input.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Done in pdf_section_extractor.py\n",
    "\n",
    "Pulls thesis/growth/risk/valuation/earnings blocks using simple\n",
    "heading heuristics, then optionally chunks text for BERT-friendly input.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff4a87",
   "metadata": {},
   "source": [
    "**SEC + TSEC Filings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6616f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      LULU\n",
      "1      LULU\n",
      "2      LULU\n",
      "3      LULU\n",
      "4      LULU\n",
      "       ... \n",
      "141     NKE\n",
      "142     NKE\n",
      "143     NKE\n",
      "144     NKE\n",
      "145     NKE\n",
      "Name: ticker, Length: 146, dtype: object\n",
      "0       LULU\n",
      "1       LULU\n",
      "2       LULU\n",
      "3       LULU\n",
      "4       LULU\n",
      "        ... \n",
      "1974     NKE\n",
      "1975     NKE\n",
      "1976     NKE\n",
      "1977     NKE\n",
      "1978     NKE\n",
      "Name: ticker, Length: 1979, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from secedgar import filings, FilingType\n",
    "\n",
    "\"\"\"\n",
    "# 8K filings for Nike and Lululemon (tickers \"nke\" and \"lulu\")\n",
    "my_filings_8k = filings(cik_lookup=[\"nke\",\"lulu\"],\n",
    "                     filing_type=FilingType.FILING_8K,\n",
    "                     user_agent=\"Simon Kurono (simonkurono@gmail.com)\")\n",
    "\n",
    "my_filings_8k.save('./nlp/raw_data/sec_filings_8k')\n",
    "\n",
    "# 10Q filings for Nike and Lululemon (tickers \"nke\" and \"lulu\")\n",
    "my_filings_8k = filings(cik_lookup=[\"nke\",\"lulu\"],\n",
    "                     filing_type=FilingType.FILING_10Q,\n",
    "                     user_agent=\"Simon Kurono (simonkurono@gmail.com)\")\n",
    "\n",
    "my_filings_8k.save('./nlp/raw_data/sec_filings_10q')\n",
    "\"\"\"\n",
    "\n",
    "#Standardize tickers in SEC files\n",
    "\n",
    "df_8k = pd.read_csv('./processed_data/final/sec_filings_extracted_8k.csv')\n",
    "df_10q = pd.read_csv('./processed_data/final/sec_filings_extracted_10q.csv')\n",
    "#print(df_8k.head())\n",
    "\n",
    "\"\"\"\n",
    "print(df_8k[\"ticker\"])\n",
    "print(df_8k[\"source_file\"])\n",
    "print(df_10k[\"ticker\"])\n",
    "print(df_10k[\"source_file\"])\n",
    "\"\"\"\n",
    "\n",
    "df_8k.loc[df_8k[\"source_file\"].str.contains(\"lulu\", case=False, na=False), \"ticker\"] = \"LULU\"\n",
    "df_8k.loc[df_8k[\"source_file\"].str.contains(\"nke\",  case=False, na=False), \"ticker\"] = \"NKE\"\n",
    "\n",
    "\n",
    "print(df_8k[\"ticker\"])\n",
    "\n",
    "df_10q.loc[df_10q[\"source_file\"].str.contains(\"lulu\", case=False, na=False), \"ticker\"] = \"LULU\"\n",
    "df_10q.loc[df_10q[\"source_file\"].str.contains(\"nke\", case=False, na=False), \"ticker\"] = \"NKE\"\n",
    "\n",
    "print(df_10q[\"ticker\"])\n",
    "\n",
    "#save\n",
    "df_8k.to_csv('./processed_data/final/sec_filings_extracted_8k.csv')\n",
    "df_10q.to_csv('./processed_data/final/sec_filings_extracted_10q.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63ba94",
   "metadata": {},
   "source": [
    "**Compile All Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skurono\\Desktop\\SCMC\\scmc-2025\\nlp\n",
      "                          id                report_id ticker  company  date  \\\n",
      "0  Aritzia-Inc-Q1-2025-MDA-0  Aritzia-Inc-Q1-2025-MDA    ATZ      NaN   NaN   \n",
      "1  Aritzia-Inc-Q1-2025-MDA-1  Aritzia-Inc-Q1-2025-MDA    ATZ      NaN   NaN   \n",
      "2  Aritzia-Inc-Q1-2025-MDA-2  Aritzia-Inc-Q1-2025-MDA    ATZ      NaN   NaN   \n",
      "3  Aritzia-Inc-Q1-2025-MDA-3  Aritzia-Inc-Q1-2025-MDA    ATZ      NaN   NaN   \n",
      "4  Aritzia-Inc-Q1-2025-MDA-4  Aritzia-Inc-Q1-2025-MDA    ATZ      NaN   NaN   \n",
      "\n",
      "       source doc_type  item section_type  section_heading  chunk_index  \\\n",
      "0  mda_canada     md&a   NaN         md&a              NaN            0   \n",
      "1  mda_canada     md&a   NaN         md&a              NaN            1   \n",
      "2  mda_canada     md&a   NaN         md&a              NaN            2   \n",
      "3  mda_canada     md&a   NaN         md&a              NaN            3   \n",
      "4  mda_canada     md&a   NaN         md&a              NaN            4   \n",
      "\n",
      "   page_start  page_end                                               text  \\\n",
      "0           1         3  First Quarter Ended June 2, 2024 July 11, 2024...   \n",
      "1           3         3  BASIS OF PRESENTATION Our audited annual conso...   \n",
      "2           3         4  Our Approach Aritzia means style, not trend, a...   \n",
      "3           4         4  Q1 2025 For Q1 2025, compared to Q1 2024: ‚Äì Ne...   \n",
      "4           4         5  See the sections below entitled ‚ÄúHow We Assess...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\sec_can\\atz_q_mda\\Aritzia-Inc-Q1-...  \n",
      "1  nlp\\raw_data\\sec_can\\atz_q_mda\\Aritzia-Inc-Q1-...  \n",
      "2  nlp\\raw_data\\sec_can\\atz_q_mda\\Aritzia-Inc-Q1-...  \n",
      "3  nlp\\raw_data\\sec_can\\atz_q_mda\\Aritzia-Inc-Q1-...  \n",
      "4  nlp\\raw_data\\sec_can\\atz_q_mda\\Aritzia-Inc-Q1-...  \n",
      "       company  date  item  section_heading  chunk_index  page_start  \\\n",
      "count      0.0   0.0   0.0              0.0   279.000000  279.000000   \n",
      "mean       NaN   NaN   NaN              NaN    23.125448   14.365591   \n",
      "std        NaN   NaN   NaN              NaN    14.057514    7.395485   \n",
      "min        NaN   NaN   NaN              NaN     0.000000    1.000000   \n",
      "25%        NaN   NaN   NaN              NaN    11.000000    8.000000   \n",
      "50%        NaN   NaN   NaN              NaN    23.000000   14.000000   \n",
      "75%        NaN   NaN   NaN              NaN    34.000000   20.000000   \n",
      "max        NaN   NaN   NaN              NaN    53.000000   30.000000   \n",
      "\n",
      "         page_end  \n",
      "count  279.000000  \n",
      "mean    14.810036  \n",
      "std      7.374595  \n",
      "min      2.000000  \n",
      "25%      9.000000  \n",
      "50%     15.000000  \n",
      "75%     21.000000  \n",
      "max     30.000000  \n",
      "                                       id  \\\n",
      "0  4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60-0   \n",
      "1  44127e76-4d51-3341-91f7-75a151c327de-0   \n",
      "2  d27b1692-49b5-37f9-8f7f-170607a64bc6-0   \n",
      "3  adf56cf4-fc14-38e2-8b57-4bdc4485cefa-0   \n",
      "4  45fdbd44-5798-37fc-9b78-dbf1bb62b6f0-0   \n",
      "\n",
      "                              report_id  ticker  company  \\\n",
      "0  4f8f8b55-7a15-3280-aefa-2ba5fe1c5d60  ATZ.TO      NaN   \n",
      "1  44127e76-4d51-3341-91f7-75a151c327de  ATZ.TO      NaN   \n",
      "2  d27b1692-49b5-37f9-8f7f-170607a64bc6  ATZ.TO      NaN   \n",
      "3  adf56cf4-fc14-38e2-8b57-4bdc4485cefa  ATZ.TO      NaN   \n",
      "4  45fdbd44-5798-37fc-9b78-dbf1bb62b6f0  ATZ.TO      NaN   \n",
      "\n",
      "                        date    source doc_type  item section_type  \\\n",
      "0  2025-12-22 13:58:54+00:00  yfinance    STORY   NaN         news   \n",
      "1  2025-12-22 12:38:14+00:00  yfinance    STORY   NaN         news   \n",
      "2  2025-12-19 19:23:11+00:00  yfinance    STORY   NaN         news   \n",
      "3  2025-12-12 12:35:50+00:00  yfinance    STORY   NaN         news   \n",
      "4  2025-12-05 17:49:06+00:00  yfinance    STORY   NaN         news   \n",
      "\n",
      "                                     section_heading  chunk_index  page_start  \\\n",
      "0  The investing winners and losers that made or ...            0         NaN   \n",
      "1  TSX Value Picks Including Aritzia And Two Othe...            0         NaN   \n",
      "2  Stifel Canada Names Gildan, KITS, and Couche-T...            0         NaN   \n",
      "3  3 TSX Growth Stocks With Up To 22% Insider Own...            0         NaN   \n",
      "4  Tech Tactics: Aritzia Taps Nedap‚Äôs RFID Platfo...            0         NaN   \n",
      "\n",
      "   page_end                                               text  \\\n",
      "0       NaN  Despite turmoil from the trade war, most globa...   \n",
      "1       NaN  As 2025 draws to a close, the Canadian market ...   \n",
      "2       NaN  Stifel Canada said Friday its best ideas for C...   \n",
      "3       NaN  As we approach the end of 2025, Canadian marke...   \n",
      "4       NaN  Aritzia utilizes Nedap's RFID platform to stre...   \n",
      "\n",
      "                                         source_file  \n",
      "0  https://ca.finance.yahoo.com/news/were-investi...  \n",
      "1  https://finance.yahoo.com/news/tsx-value-picks...  \n",
      "2  https://finance.yahoo.com/news/stifel-canada-n...  \n",
      "3  https://finance.yahoo.com/news/3-tsx-growth-st...  \n",
      "4  https://sourcingjournal.com/topics/technology/...  \n",
      "       company  item  chunk_index  page_start  page_end\n",
      "count      0.0   0.0         30.0         0.0       0.0\n",
      "mean       NaN   NaN          0.0         NaN       NaN\n",
      "std        NaN   NaN          0.0         NaN       NaN\n",
      "min        NaN   NaN          0.0         NaN       NaN\n",
      "25%        NaN   NaN          0.0         NaN       NaN\n",
      "50%        NaN   NaN          0.0         NaN       NaN\n",
      "75%        NaN   NaN          0.0         NaN       NaN\n",
      "max        NaN   NaN          0.0         NaN       NaN\n",
      "   7ac88f377541dfa1a4647f81055416b1  NKE Unnamed: 2  2025-12-05T22:06:20Z  \\\n",
      "0  8298c16f954431dcb0da138467024fdd  NKE        NaN  2025-12-17T18:47:56Z   \n",
      "1  7cd991eba9bad7579b584ca9daa2aad3  NKE        NaN  2025-12-25T15:36:47Z   \n",
      "2  777093cbbccf2c9e9cba482a8cadb43d  NKE        NaN  2025-12-22T17:01:51Z   \n",
      "3  7e8e76df9eabd632df3989fdf36f7b65  NKE        NaN  2025-12-07T15:00:02Z   \n",
      "4  fca45a1010c5d7b93d7c6143e7b0e627  NKE        NaN  2025-12-21T15:44:50Z   \n",
      "\n",
      "   news  news_article  Unnamed: 6 news.1  \\\n",
      "0  news  news_article         NaN   news   \n",
      "1  news  news_article         NaN   news   \n",
      "2  news  news_article         NaN   news   \n",
      "3  news  news_article         NaN   news   \n",
      "4  news  news_article         NaN   news   \n",
      "\n",
      "                NIKE, Inc. (NKE): A¬†Bear¬†Case Theory  0  Unnamed: 10  \\\n",
      "0  Guggenheim Initiates Nike (NKE) at Buy on Marg...  0          NaN   \n",
      "1      Nike (NKE) Jumps 4.6% as Tim Cook Hikes Stake  0          NaN   \n",
      "2  Earnings Disappointment Sends Nike Stock Below...  0          NaN   \n",
      "3  As Nike Shakes Up Its C-Suite, Should You Buy,...  0          NaN   \n",
      "4               Jim Cramer Highlights Nike Struggles  0          NaN   \n",
      "\n",
      "   Unnamed: 11  \\\n",
      "0          NaN   \n",
      "1          NaN   \n",
      "2          NaN   \n",
      "3          NaN   \n",
      "4          NaN   \n",
      "\n",
      "  We came across a bearish thesis on NIKE, Inc. on Uncle Stock Notes‚Äôs Substack. In this article, we will summarize the bulls‚Äô thesis on NKE. NIKE, Inc.'s share was trading at $65.39 as of December 1st. NKE‚Äôs trailing and forward P/E were 33.53 and 40.32, respectively according to Yahoo Finance.\\nPixabay/Public Domain\\nNike‚Äôs Q1 FY2026 results underscore a company in transition, struggling to find footing amid mounting competitive and operational pressures. While total revenue rose a modest 1% year-over-year to $11.7 billion, the composition of growth revealed deeper issues‚ÄîNorth American revenue declined 3%, and Nike Direct sales fell 4%, undermining years of effort to build a high-margin D2C model. Instead, wholesale sales grew 7%, signaling a tactical reversal as Nike leans on retail partners to offset direct-channel weakness. This shift, though necessary in the short term, highlights the strain on Nike‚Äôs once-vaunted brand power and consumer connection.\\nGross margin deterioration was the most alarming signal, plunging 320 basis points to 42.2% due to elevated product costs, unfavorable currency effects, and widespread discounting to clear excess inventory. The erosion of pricing power underscores the weakening of Nike‚Äôs brand premium‚Äîa critical concern for a company long defined by its aspirational positioning. Management‚Äôs ‚ÄúWin Now‚Äù initiative, intended to reignite core categories such as running and basketball, remains in early stages, with little evidence yet of meaningful financial impact.\\nRegionally, softness in Greater China and continued weakness in North America offset moderate growth in EMEA and APLA. The company‚Äôs reluctance to issue guidance reflects uncertainty about the near-term outlook, as inventory challenges, macro headwinds, and rising competition from brands like Hoka and On weigh on visibility. Though Nike‚Äôs global scale, supply chain strength, and distribution network remain significant advantages, the quarter revealed a brand in search of renewed identity and strategic clarity. For now, sentiment skews bearish as the company‚Äôs transition from stagnation to sustainable growth remains incomplete.\\nPreviously, we covered a bullish thesis on NIKE, Inc. (NKE) by Any_Chocolate6194 in May 2025, which highlighted the company‚Äôs brand dominance, leadership renewal, and long-term recovery potential. The company‚Äôs stock price has appreciated by approximately 5.33% since our coverage. This is because the recovery thesis has yet to fully play out. Uncle Stock Notes shares a contrarian but emphasizes near-term execution and D2C challenges.  \\\n",
      "0  NIKE, Inc. (NYSE:NKE) is included among the 12...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "1  We recently published 10 Stocks Lighting Up Ma...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "2  Nike (NKE) shares crashed over 10% on Dec. 19 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "3  NIKE (NKE), the global icon of sport and stree...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "4  NIKE, Inc. (NYSE:NKE) is one of the tech and c...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "\n",
      "  https://finance.yahoo.com/news/nike-inc-nke-bear-case-220620409.html  \n",
      "0  https://finance.yahoo.com/news/guggenheim-init...                    \n",
      "1  https://finance.yahoo.com/news/nike-nke-jumps-...                    \n",
      "2  https://www.barchart.com/story/news/36746721/e...                    \n",
      "3  https://www.barchart.com/story/news/36497105/a...                    \n",
      "4  https://finance.yahoo.com/news/jim-cramer-high...                    \n",
      "       Unnamed: 6      0  Unnamed: 10  Unnamed: 11\n",
      "count         0.0  155.0          0.0          0.0\n",
      "mean          NaN    0.0          NaN          NaN\n",
      "std           NaN    0.0          NaN          NaN\n",
      "min           NaN    0.0          NaN          NaN\n",
      "25%           NaN    0.0          NaN          NaN\n",
      "50%           NaN    0.0          NaN          NaN\n",
      "75%           NaN    0.0          NaN          NaN\n",
      "max           NaN    0.0          NaN          NaN\n",
      "                                                  id  \\\n",
      "0  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "1  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "2  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "3  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "4  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "\n",
      "                                           report_id ticker  company  date  \\\n",
      "0  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ      NaN   NaN   \n",
      "1  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ      NaN   NaN   \n",
      "2  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ      NaN   NaN   \n",
      "3  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ      NaN   NaN   \n",
      "4  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ      NaN   NaN   \n",
      "\n",
      "      source       doc_type  item section_type  \\\n",
      "0  bloomberg  research_note   NaN       thesis   \n",
      "1  bloomberg  research_note   NaN       growth   \n",
      "2  bloomberg  research_note   NaN       growth   \n",
      "3  bloomberg  research_note   NaN       growth   \n",
      "4  bloomberg  research_note   NaN       growth   \n",
      "\n",
      "                                     section_heading  chunk_index  page_start  \\\n",
      "0  Aritzia Sales Could Double on US Expansion, Gr...            0           1   \n",
      "1    1. Setting Up Significant White Space Potential            1           1   \n",
      "2  Private Brands Are 95% of Sales: Wilfred 28%, ...            2           1   \n",
      "3    2. Margin Can Increase at Least 500 Bps in 2024            3           1   \n",
      "4  3. Digital Sales Poised to Double by 2027 (Cor...            4           2   \n",
      "\n",
      "   page_end                                               text  \\\n",
      "0         1  ( bloomberg intelligence ) - - aritzia has a l...   \n",
      "1         1  aritzia's plan to lift revenue 60 % to c $ 3. ...   \n",
      "2         1  26 %, super puff 7 - 8 %, denim forum, sunday ...   \n",
      "3         2  aritzia's ebitda margin could expand by 500 bp...   \n",
      "4         2  aritzia's c $ 800 million of digital sales cou...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "1  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "2  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "3  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "4  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "       company  date  item  chunk_index   page_start     page_end\n",
      "count      0.0   0.0   0.0  3178.000000  3178.000000  3178.000000\n",
      "mean       NaN   NaN   NaN    24.095972     9.231907    10.274386\n",
      "std        NaN   NaN   NaN    21.194898     9.563401    10.006431\n",
      "min        NaN   NaN   NaN     0.000000     1.000000     1.000000\n",
      "25%        NaN   NaN   NaN     7.000000     3.000000     3.000000\n",
      "50%        NaN   NaN   NaN    19.000000     6.000000     7.000000\n",
      "75%        NaN   NaN   NaN    35.000000    12.000000    14.000000\n",
      "max        NaN   NaN   NaN   113.000000    63.000000    67.000000\n",
      "                                                  id  \\\n",
      "0  Aritzia_-_NCIB_2025_-_Announcement_Press_Relea...   \n",
      "1  Aritzia_-_NCIB_2025_-_Announcement_Press_Relea...   \n",
      "2  Aritzia_-_NCIB_2025_-_Announcement_Press_Relea...   \n",
      "3  Aritzia_-_NCIB_2025_-_Announcement_Press_Relea...   \n",
      "4  Aritzia_-_NCIB_2025_-_Announcement_Press_Relea...   \n",
      "\n",
      "                                          report_id ticker       company  \\\n",
      "0  Aritzia_-_NCIB_2025_-_Announcement_Press_Release    ATZ  Aritzia Inc.   \n",
      "1  Aritzia_-_NCIB_2025_-_Announcement_Press_Release    ATZ  Aritzia Inc.   \n",
      "2  Aritzia_-_NCIB_2025_-_Announcement_Press_Release    ATZ  Aritzia Inc.   \n",
      "3  Aritzia_-_NCIB_2025_-_Announcement_Press_Release    ATZ  Aritzia Inc.   \n",
      "4  Aritzia_-_NCIB_2025_-_Announcement_Press_Release    ATZ  Aritzia Inc.   \n",
      "\n",
      "   date         source           doc_type  item     section_type  \\\n",
      "0   NaN  press_release  ncib_announcement   NaN  capital_returns   \n",
      "1   NaN  press_release  ncib_announcement   NaN  capital_returns   \n",
      "2   NaN  press_release  ncib_announcement   NaN      boilerplate   \n",
      "3   NaN  press_release  ncib_announcement   NaN            other   \n",
      "4   NaN  press_release  ncib_announcement   NaN      boilerplate   \n",
      "\n",
      "                              section_heading  chunk_index  page_start  \\\n",
      "0  ARITZIA ANNOUNCES NORMAL COURSE ISSUER BID            0           1   \n",
      "1  ARITZIA ANNOUNCES NORMAL COURSE ISSUER BID            1           1   \n",
      "2                               About Aritzia            0           1   \n",
      "3                                Our Approach            0           1   \n",
      "4                 Forward-looking Information            0           2   \n",
      "\n",
      "   page_end                                               text  \\\n",
      "0         1  VANCOUVER, May 5, 2025 ‚Äì Aritzia Inc. (\"Aritzi...   \n",
      "1         1  As at March 2, 2025, the Company had approxima...   \n",
      "2         1  Aritzia is a design house with an innovative g...   \n",
      "3         2  Aritzia means style, not trend, and quality ov...   \n",
      "4         3  Certain statements made in this press release ...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\sec_can\\news_releases\\Aritzia_-_N...  \n",
      "1  nlp\\raw_data\\sec_can\\news_releases\\Aritzia_-_N...  \n",
      "2  nlp\\raw_data\\sec_can\\news_releases\\Aritzia_-_N...  \n",
      "3  nlp\\raw_data\\sec_can\\news_releases\\Aritzia_-_N...  \n",
      "4  nlp\\raw_data\\sec_can\\news_releases\\Aritzia_-_N...  \n",
      "       date  item  chunk_index  page_start    page_end\n",
      "count   0.0   0.0   189.000000  189.000000  189.000000\n",
      "mean    NaN   NaN     1.095238    5.920635    6.486772\n",
      "std     NaN   NaN     2.096458    3.985218    3.883419\n",
      "min     NaN   NaN     0.000000    1.000000    1.000000\n",
      "25%     NaN   NaN     0.000000    2.000000    3.000000\n",
      "50%     NaN   NaN     0.000000    5.000000    7.000000\n",
      "75%     NaN   NaN     1.000000    9.000000   10.000000\n",
      "max     NaN   NaN     9.000000   14.000000   14.000000\n",
      "                       id             report_id ticker  company        date  \\\n",
      "0  0000909567-07-001490-0  0000909567-07-001490    TXT      NaN  2007-11-29   \n",
      "1  0000909567-08-000377-0  0000909567-08-000377    TXT      NaN  2008-04-02   \n",
      "2  0000909567-08-001002-0  0000909567-08-001002    TXT      NaN  2008-09-11   \n",
      "3  0000909567-08-001269-0  0000909567-08-001269    TXT      NaN  2008-12-11   \n",
      "4  0000945234-07-000574-0  0000945234-07-000574    TXT      NaN  2007-09-10   \n",
      "\n",
      "  source doc_type  item section_type  \\\n",
      "0    sec      8-K  2.02      results   \n",
      "1    sec      8-K  2.02      results   \n",
      "2    sec      8-K  2.02      results   \n",
      "3    sec      8-K  2.02      results   \n",
      "4    sec      8-K  2.02      results   \n",
      "\n",
      "                                     section_heading  chunk_index  page_start  \\\n",
      "0  Results of Operations and Financial Condition ...            0         NaN   \n",
      "1  Results of Operations and Financial Condition ...            0         NaN   \n",
      "2  Results of Operations and Financial Condition ...            0         NaN   \n",
      "3  Results of Operations and Financial Condition ...            0         NaN   \n",
      "4  Results of Operations and Financial Condition ...            0         NaN   \n",
      "\n",
      "   page_end                                               text  \\\n",
      "0       NaN  On November 29, 2007, lululemon athletica inc....   \n",
      "1       NaN  In\\naddition to reporting financial results in...   \n",
      "2       NaN  On September 11, 2008, lululemon athletica inc...   \n",
      "3       NaN  On December 11, 2008, lululemon athletica inc....   \n",
      "4       NaN  On September 10, 2007, lululemon athletica inc...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\sec_filings_8k\\lulu\\8-K\\000090956...  \n",
      "1  nlp\\raw_data\\sec_filings_8k\\lulu\\8-K\\000090956...  \n",
      "2  nlp\\raw_data\\sec_filings_8k\\lulu\\8-K\\000090956...  \n",
      "3  nlp\\raw_data\\sec_filings_8k\\lulu\\8-K\\000090956...  \n",
      "4  nlp\\raw_data\\sec_filings_8k\\lulu\\8-K\\000094523...  \n",
      "       company        item  chunk_index  page_start  page_end\n",
      "count      0.0  146.000000   146.000000         0.0       0.0\n",
      "mean       NaN    4.747945     0.082192         NaN       NaN\n",
      "std        NaN    2.797217     0.462480         NaN       NaN\n",
      "min        NaN    2.020000     0.000000         NaN       NaN\n",
      "25%        NaN    2.020000     0.000000         NaN       NaN\n",
      "50%        NaN    2.020000     0.000000         NaN       NaN\n",
      "75%        NaN    8.010000     0.000000         NaN       NaN\n",
      "max        NaN    8.010000     4.000000         NaN       NaN\n",
      "                       id             report_id ticker  company        date  \\\n",
      "0  0000909567-08-001001-0  0000909567-08-001001    TXT      NaN  2008-09-11   \n",
      "1  0000909567-08-001001-1  0000909567-08-001001    TXT      NaN  2008-09-11   \n",
      "2  0000909567-08-001001-2  0000909567-08-001001    TXT      NaN  2008-09-11   \n",
      "3  0000909567-08-001001-3  0000909567-08-001001    TXT      NaN  2008-09-11   \n",
      "4  0000909567-08-001001-4  0000909567-08-001001    TXT      NaN  2008-09-11   \n",
      "\n",
      "  source doc_type item section_type section_heading  chunk_index  page_start  \\\n",
      "0    sec     10-Q    1     business        Business            0         NaN   \n",
      "1    sec     10-Q    1     business        Business            1         NaN   \n",
      "2    sec     10-Q    1     business        Business            2         NaN   \n",
      "3    sec     10-Q    1     business        Business            3         NaN   \n",
      "4    sec     10-Q    1     business        Business            4         NaN   \n",
      "\n",
      "   page_end                                               text  \\\n",
      "0       NaN  During the second quarter of fiscal 2008, foll...   \n",
      "1       NaN  During the second quarter of fiscal 2008, afte...   \n",
      "2       NaN  We believe this claim is without merit and are...   \n",
      "3       NaN  Additionally, we, or the subsidiary employing ...   \n",
      "4       NaN  Unrealized gains and losses on items\\n for whi...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\sec_filings_10q\\lulu\\10-Q\\0000909...  \n",
      "1  nlp\\raw_data\\sec_filings_10q\\lulu\\10-Q\\0000909...  \n",
      "2  nlp\\raw_data\\sec_filings_10q\\lulu\\10-Q\\0000909...  \n",
      "3  nlp\\raw_data\\sec_filings_10q\\lulu\\10-Q\\0000909...  \n",
      "4  nlp\\raw_data\\sec_filings_10q\\lulu\\10-Q\\0000909...  \n",
      "       company  chunk_index  page_start  page_end\n",
      "count      0.0  1979.000000         0.0       0.0\n",
      "mean       NaN     2.177362         NaN       NaN\n",
      "std        NaN     1.810366         NaN       NaN\n",
      "min        NaN     0.000000         NaN       NaN\n",
      "25%        NaN     1.000000         NaN       NaN\n",
      "50%        NaN     2.000000         NaN       NaN\n",
      "75%        NaN     3.000000         NaN       NaN\n",
      "max        NaN    11.000000         NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "#Convert all csvs to dataframes\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "atz_mda_df = pd.read_csv(\"processed_data/final/atz_mda_chunks.csv\")\n",
    "print(atz_mda_df.head())\n",
    "print(atz_mda_df.describe())\n",
    "\n",
    "yfinance_news_df = pd.read_csv(\"processed_data/final/financial_news_dataset.csv\")\n",
    "print(yfinance_news_df.head())\n",
    "print(yfinance_news_df.describe())\n",
    "\n",
    "newsapi_df = pd.read_csv(\"processed_data/final/news_data_final.csv\")\n",
    "print(newsapi_df.head())\n",
    "print(newsapi_df.describe())\n",
    "\n",
    "equity_report_df = pd.read_csv(\"processed_data/final/pdf_sections_chunks.csv\")\n",
    "print(equity_report_df.head())\n",
    "print(equity_report_df.describe())\n",
    "\n",
    "atz_press_release_df = pd.read_csv(\"processed_data/final/press_releases_chunks.csv\")\n",
    "print(atz_press_release_df.head())\n",
    "print(atz_press_release_df.describe())\n",
    "\n",
    "sec_8k_df = pd.read_csv(\"processed_data/final/sec_filings_extracted_8k.csv\")\n",
    "print(sec_8k_df.head())\n",
    "print(sec_8k_df.describe())\n",
    "\n",
    "sec_10q = pd.read_csv(\"processed_data/final/sec_filings_extracted_10q.csv\")\n",
    "print(sec_10q.head())\n",
    "print(sec_10q.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f0f5c",
   "metadata": {},
   "source": [
    "**Combine datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c17cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loaded pdf_sections_chunks.csv: 3178 rows\n",
      "[info] loaded atz_mda_chunks.csv: 279 rows\n",
      "[info] loaded sec_filings_extracted_10q.csv: 1979 rows\n",
      "[info] loaded sec_filings_extracted_8k.csv: 146 rows\n",
      "[info] loaded press_releases_chunks.csv: 189 rows\n",
      "[info] loaded financial_news_dataset.csv: 30 rows\n",
      "[info] loaded news_data_final.csv: 156 rows\n",
      "[done] master rows=5955 -> processed_data/final/master_corpus.csv\n",
      "                                                  id  \\\n",
      "0  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "1  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "2  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "3  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "4  20231103_Digital_Sales_Poised_to_Double_by_202...   \n",
      "\n",
      "                                           report_id ticker company date  \\\n",
      "0  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ     NaN  NaN   \n",
      "1  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ     NaN  NaN   \n",
      "2  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ     NaN  NaN   \n",
      "3  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ     NaN  NaN   \n",
      "4  20231103_Digital_Sales_Poised_to_Double_by_202...    ATZ     NaN  NaN   \n",
      "\n",
      "      source       doc_type item section_type  \\\n",
      "0  bloomberg  research_note  NaN       thesis   \n",
      "1  bloomberg  research_note  NaN       growth   \n",
      "2  bloomberg  research_note  NaN       growth   \n",
      "3  bloomberg  research_note  NaN       growth   \n",
      "4  bloomberg  research_note  NaN       growth   \n",
      "\n",
      "                                     section_heading  chunk_index  page_start  \\\n",
      "0  Aritzia Sales Could Double on US Expansion, Gr...          0.0         1.0   \n",
      "1    1. Setting Up Significant White Space Potential          1.0         1.0   \n",
      "2  Private Brands Are 95% of Sales: Wilfred 28%, ...          2.0         1.0   \n",
      "3    2. Margin Can Increase at Least 500 Bps in 2024          3.0         1.0   \n",
      "4  3. Digital Sales Poised to Double by 2027 (Cor...          4.0         2.0   \n",
      "\n",
      "  page_end                                               text  \\\n",
      "0        1  ( bloomberg intelligence ) - - aritzia has a l...   \n",
      "1        1  aritzia's plan to lift revenue 60 % to c $ 3. ...   \n",
      "2        1  26 %, super puff 7 - 8 %, denim forum, sunday ...   \n",
      "3        2  aritzia's ebitda margin could expand by 500 bp...   \n",
      "4        2  aritzia's c $ 800 million of digital sales cou...   \n",
      "\n",
      "                                         source_file  \n",
      "0  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "1  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "2  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "3  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n",
      "4  nlp\\raw_data\\bloomberg\\atz_research\\20231103_D...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CANONICAL_FIELDS = [\n",
    "    'id', 'report_id', 'ticker', 'company', 'date', 'source', 'doc_type', 'item', 'section_type', 'section_heading', 'chunk_index', 'page_start', 'page_end', 'text', 'source_file'\n",
    "]\n",
    "\n",
    "base = Path('processed_data/final')\n",
    "data_paths = [\n",
    "    base / 'pdf_sections_chunks.csv',\n",
    "    base / 'atz_mda_chunks.csv',\n",
    "    base / 'sec_filings_extracted_10q.csv',\n",
    "    base / 'sec_filings_extracted_8k.csv',\n",
    "    base / 'press_releases_chunks.csv',\n",
    "    base / 'financial_news_dataset.csv',\n",
    "    base / 'news_data_final.csv',\n",
    "]\n",
    "\n",
    "frames = []\n",
    "for p in data_paths:\n",
    "    if not p.exists():\n",
    "        print(f'[warn] missing {p}')\n",
    "        continue\n",
    "    master = pd.read_csv(p)\n",
    "    for col in CANONICAL_FIELDS:\n",
    "        if col not in master.columns:\n",
    "            master[col] = ''\n",
    "    master = master.reindex(columns=CANONICAL_FIELDS)\n",
    "    frames.append(master)\n",
    "    print(f'[info] loaded {p.name}: {len(master)} rows')\n",
    "\n",
    "if frames:\n",
    "    master = pd.concat(frames, ignore_index=True)\n",
    "    master = master[master['text'].astype(str).str.len() > 0]\n",
    "    master = master.drop_duplicates(subset=['text', 'source_file', 'section_heading'])\n",
    "else:\n",
    "    master = pd.DataFrame(columns=CANONICAL_FIELDS)\n",
    "\n",
    "out_path = base / 'master_corpus.csv'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "master.to_csv(out_path, index=False)\n",
    "print(f'[done] master rows={len(master)} -> {out_path}')\n",
    "print(master.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff82d53",
   "metadata": {},
   "source": [
    "**Clean Master and ensure no mistakes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATZ' 'LULU' 'NKE' nan 'nike' 'lulu']\n",
      "['research_note' 'md&a' '10-Q' '8-K' 'ncib_announcement'\n",
      " 'earnings_release' 'agm_results' 'STORY' nan]\n",
      "0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "5950    False\n",
      "5951    False\n",
      "5952    False\n",
      "5953    False\n",
      "5954    False\n",
      "Length: 5955, dtype: bool\n",
      "        Unnamed: 0  chunk_index   page_start\n",
      "count  5955.000000  5799.000000  3644.000000\n",
      "mean   2978.404702    15.083635     9.451976\n",
      "std    1719.862629    19.333507     9.340196\n",
      "min       0.000000     0.000000     1.000000\n",
      "25%    1489.500000     2.000000     3.000000\n",
      "50%    2979.000000     6.000000     7.000000\n",
      "75%    4467.500000    24.000000    13.000000\n",
      "max    5956.000000   113.000000    63.000000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "master = pd.read_csv('./processed_data/backups/master_corpus.csv')\n",
    "print(master[\"ticker\"].unique())\n",
    "print(master[\"doc_type\"].unique())\n",
    "\n",
    "\n",
    "print(master.duplicated(\"text\"))\n",
    "print(master.describe())\n",
    "#master.to_csv('./processed_data/backups/master_corpus_no_dupes.csv')\n",
    "\n",
    "#fix ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfa00a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5878\n",
      "Columns: ['Unnamed: 0', 'id', 'report_id', 'ticker', 'company', 'date', 'source', 'doc_type', 'item', 'section_type', 'section_heading', 'chunk_index', 'page_start', 'page_end', 'text', 'source_file', 'is_empty', 'char_len', 'html_like', 'json_like', 'high_symbol_ratio']\n",
      "\n",
      "Empty rows: 0\n",
      "Rows < 20 chars: 100\n",
      "Rows > 2000 chars: 656\n",
      "Max length: 3094\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>% )</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>1q26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>mix )</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>peers</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>peers</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  char_len\n",
       "1186    % )         3\n",
       "2525   1q26         4\n",
       "1189  mix )         5\n",
       "2110  peers         5\n",
       "2046  peers         5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>Our expansive and diverse range of fashion app...</td>\n",
       "      <td>2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>As at August 31, 2025, the fair value of the e...</td>\n",
       "      <td>2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>We manage liquidity risk by continuously monit...</td>\n",
       "      <td>2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>Three Months Ended \\n August 31, \\n __________...</td>\n",
       "      <td>3063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>Three Months Ended \\n August 31, \\n __________...</td>\n",
       "      <td>3094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  char_len\n",
       "3185  Our expansive and diverse range of fashion app...      2617\n",
       "3351  As at August 31, 2025, the fair value of the e...      2627\n",
       "3398  We manage liquidity risk by continuously monit...      2674\n",
       "5202  Three Months Ended \\n August 31, \\n __________...      3063\n",
       "5169  Three Months Ended \\n August 31, \\n __________...      3094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate text rows: 1240\n",
      "\n",
      "Most repeated texts:\n",
      "  14x -> events ), with 4q net revenue ( ex - 53rd week ) expected to increase 28 - 31 %, according to guidance gross margin expa...\n",
      "  13x -> expected to gain 20 - 23. 5 %, based on guidance gross margin expanded 359 bps to 43. 8 %, 250 bps ahead of consensus, d...\n",
      "  13x -> post - 1q earnings outlook : aritzia could surpass 2q guidance for 19 - 22 % sales growth ( vs. analysts'20. 8 % ) and i...\n",
      "  13x -> expected to increase 19 - 22 %, according to guidance gross margin expanded 320 bps to 47. 2 %, 20 bps ahead of consensu...\n",
      "  13x -> to increase 3 - 7 %, based on guidance gross margin expanded 522 bps to 40. 2 %, 71 bps ahead of consensus'39. 5 %, on l...\n",
      "  13x -> rise 7 - 10 %, based on guidance gross margin climbed 510 bps to 44 %, 64 bps ahead of consensus'43. 4 %, on lower markd...\n",
      "  12x -> post - 3q earnings outlook : aritzia could meet the high end of 4q sales guidance of 31 % growth ( adjusting for the ext...\n",
      "  12x -> rise 3 - 7 %, based on guidance gross margin climbed 30 bps to 38. 3 %, a tad under consensus'38. 4 %, on select price i...\n",
      "  12x -> rise 5 - 8 %, including 53rd week, based on guidance gross margin drops 180 bps ( less than 200 - bp guidance ) to 41. 5...\n",
      "  12x -> master limited partnerships ( mlps ) are pass - through entities structured as publicly listed partnerships. for tax pur...\n",
      "\n",
      "Possible HTML rows: 22\n",
      "Possible JSON blobs: 0\n",
      "Rows with many strange symbols: 11\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "TEXT_COL = \"text\"   # change if needed\n",
    "\n",
    "assert TEXT_COL in master.columns, f\"Missing column: {TEXT_COL}\"\n",
    "\n",
    "\n",
    "# --- basic integrity ---\n",
    "print(\"Rows:\", len(master))\n",
    "print(\"Columns:\", list(master.columns))\n",
    "\n",
    "# enforce string + fill NaNs\n",
    "master[TEXT_COL] = master[TEXT_COL].astype(str).fillna(\"\")\n",
    "\n",
    "# --- empty / whitespace detection ---\n",
    "master[\"is_empty\"] = master[TEXT_COL].str.strip().eq(\"\")\n",
    "print(\"\\nEmpty rows:\", master[\"is_empty\"].sum())\n",
    "\n",
    "# --- extremely short fragments ---\n",
    "master[\"char_len\"] = master[TEXT_COL].str.len()\n",
    "print(\"Rows < 20 chars:\", (master[\"char_len\"] < 20).sum())\n",
    "\n",
    "# --- extremely long rows (will truncate at 512 tokens) ---\n",
    "print(\"Rows > 2000 chars:\", (master[\"char_len\"] > 2000).sum())\n",
    "print(\"Max length:\", master[\"char_len\"].max())\n",
    "\n",
    "# peek at extremes\n",
    "display(master.sort_values(\"char_len\").head(5)[[TEXT_COL,\"char_len\"]])\n",
    "display(master.sort_values(\"char_len\").tail(5)[[TEXT_COL,\"char_len\"]])\n",
    "\n",
    "# --- duplicate detection ---\n",
    "dup_rows = master.duplicated(subset=[TEXT_COL]).sum()\n",
    "print(\"\\nDuplicate text rows:\", dup_rows)\n",
    "\n",
    "# --- repeated boilerplate detector (top 10 repeated snippets) ---\n",
    "most_common = Counter(master[TEXT_COL]).most_common(10)\n",
    "print(\"\\nMost repeated texts:\")\n",
    "for t, c in most_common:\n",
    "    if c > 5:\n",
    "        print(f\"  {c}x ->\", t[:120].replace(\"\\n\",\" \") + \"...\")\n",
    "\n",
    "# --- garbage / artifact heuristic flags ---\n",
    "def looks_like_html(s):\n",
    "    return bool(re.search(r\"<[^>]+>\", s))\n",
    "\n",
    "def looks_like_json(s):\n",
    "    return s.strip().startswith(\"{\") and s.strip().endswith(\"}\")\n",
    "\n",
    "master[\"html_like\"] = master[TEXT_COL].apply(looks_like_html)\n",
    "master[\"json_like\"] = master[TEXT_COL].apply(looks_like_json)\n",
    "\n",
    "print(\"\\nPossible HTML rows:\", master[\"html_like\"].sum())\n",
    "print(\"Possible JSON blobs:\", master[\"json_like\"].sum())\n",
    "\n",
    "# --- language / encoding artifacts (optional heuristic) ---\n",
    "master[\"high_symbol_ratio\"] = (master[TEXT_COL].str.count(r\"[^A-Za-z0-9\\s.,;:$%-]\") / master[\"char_len\"].clip(lower=1)) > 0.25\n",
    "print(\"Rows with many strange symbols:\", master[\"high_symbol_ratio\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7737d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footer rows to drop: 0\n",
      "\n",
      "Duplicate text rows: 1240\n",
      "        Unnamed: 0  chunk_index   page_start     char_len\n",
      "count  5878.000000  5722.000000  3567.000000  5878.000000\n",
      "mean   3011.858625    14.957882     9.548080  1182.447941\n",
      "std    1705.664901    19.387793     9.412726   676.646121\n",
      "min       0.000000     0.000000     1.000000     3.000000\n",
      "25%    1547.250000     2.000000     3.000000   571.000000\n",
      "50%    3017.500000     5.000000     7.000000  1399.000000\n",
      "75%    4486.750000    23.000000    13.000000  1624.000000\n",
      "max    5956.000000   113.000000    63.000000  3094.000000\n"
     ]
    }
   ],
   "source": [
    "#drop header/footer rows\n",
    "\n",
    "FOOTER_SNIPPET = \"additional resources : analyzer | bi ¬ª earnings calendar | evts ¬ª\"\n",
    "\n",
    "norm_text = (\n",
    "    master[\"text\"]\n",
    "    .fillna(\"\")\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "footer_mask = norm_text.str.contains(FOOTER_SNIPPET.lower(), regex=False)\n",
    "\n",
    "print(\"Footer rows to drop:\", footer_mask.sum())\n",
    "\n",
    "master = master[~footer_mask].copy()\n",
    "\n",
    "dup_rows = master.duplicated(subset=[TEXT_COL]).sum()\n",
    "print(\"\\nDuplicate text rows:\", dup_rows)\n",
    "\n",
    "print(master.describe())\n",
    "\n",
    "#save master\n",
    "master.to_csv('./processed_data/final/master_corpus.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
